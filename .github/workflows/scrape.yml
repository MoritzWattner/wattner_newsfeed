name: scrape-feeds

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch: {}

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Show workspace (before)
        run: |
          echo "PWD=$(pwd)"
          ls -lah

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        env:
          PYTHONUNBUFFERED: "1"
        run: |
          set -euxo pipefail
          mkdir -p data feeds logs
          python scraper.py --once

      - name: Show workspace (after)
        run: |
          echo "== data =="
          ls -lah data || true
          echo "== feeds =="
          ls -lah feeds || true
          echo "== logs =="
          ls -lah logs || true
          [ -f data/state.json ] && head -n 20 data/state.json || true

      - name: Upload logs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: logs
          path: logs
          if-no-files-found: warn

      - name: Commit state.json (handles untracked)
        run: |
          set -euxo pipefail
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          # untracked Files erkennen:
          if [ -f data/state.json ]; then
            git add -A
            git commit -m "Update state.json [skip ci]" || echo "no changes to commit"
            git push || true
          fi

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload feeds artifact (for Pages)
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./feeds

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
