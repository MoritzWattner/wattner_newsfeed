name: scrape-feeds

on:
  schedule:
    - cron: "*/60 * * * *"   # alle 60 Minuten
  workflow_dispatch: {}       # manuell startbar

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          mkdir -p data feeds logs
          python scraper.py --once

      - name: Commit updated state.json
        run: |
          if git diff --quiet -- data/state.json; then
            echo "No state changes"
          else
            git config user.name "github-actions"
            git config user.email "actions@github.com"
            git add data/state.json
            git commit -m "Update state.json [skip ci]" || true
            git push
          fi

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload feeds artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./feeds

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
